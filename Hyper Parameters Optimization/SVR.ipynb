{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyDOE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats.distributions as dist\n",
    "from sklearn.svm import SVR\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueRange = namedtuple('ValueRange', ['min', 'max'])\n",
    "\n",
    "def determinerange(values):\n",
    "    \"\"\"Determine the range of values in each dimension\"\"\"\n",
    "    return ValueRange(np.min(values, axis=0), np.max(values, axis=0))\n",
    "\n",
    "\n",
    "def linearscaletransform(values, *, range_in=None, range_out=ValueRange(0, 1), scale_only=False):\n",
    "    \"\"\"Perform a scale transformation of `values`: [range_in] --> [range_out]\"\"\"\n",
    "\n",
    "    if range_in is None:\n",
    "        range_in = determinerange(values)\n",
    "    elif not isinstance(range_in, ValueRange):\n",
    "        range_in = ValueRange(*range_in)\n",
    "\n",
    "    if not isinstance(range_out, ValueRange):\n",
    "        range_out = ValueRange(*range_out)\n",
    "\n",
    "    scale_out = range_out.max - range_out.min\n",
    "    scale_in = range_in.max - range_in.min\n",
    "\n",
    "    if scale_only:\n",
    "        scaled_values = (values / scale_in) * scale_out\n",
    "    else:\n",
    "        scaled_values = (values - range_in.min) / scale_in\n",
    "        scaled_values = (scaled_values * scale_out) + range_out.min\n",
    "\n",
    "    return scaled_values\n",
    "\n",
    "''' Support Vector Regression'''\n",
    "def _SVR(train_data,test_data , Hyper):\n",
    "    gam, reg= Hyper\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler().fit(np.r_[train_data.iloc[:,:-1].values, test_data.values])\n",
    "    gpr = sklearn.svm.SVR(kernel='rbf', C=reg ,max_iter=1500).fit( scaler.transform(train_data.iloc[:,:-1]), train_data.iloc[:,-1])\n",
    "    pred = gpr.predict(scaler.transform(test_data))\n",
    "    return gpr,pred\n",
    "\n",
    "''' Latin HyperCube Sampling Design of Experiment for Hyper_Parameters Optimization'''\n",
    "def DOE_Hyper():\n",
    "    np.random.seed(0)\n",
    "    lhd = pyDOE.lhs(n=2, samples=100, criterion='m')\n",
    "    X1, X2 = lhd[:,0], lhd[:,1] \n",
    "    X1 = linearscaletransform(X1,range_out=(0.00001,1e2))\n",
    "    X2 = linearscaletransform(X2,range_out=(0.0001 , 1000))\n",
    "    Hyper_Parameters = pd.DataFrame()\n",
    "    Hyper_Parameters['Gamma'] = pd.Series(X1)\n",
    "    Hyper_Parameters['Regularization'] = pd.Series(X2)\n",
    "    return Hyper_Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Test Data Set initially Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Training_Data_Sets\\\\train_2_1000Samples.csv\"\n",
    "train = pd.read_csv(path).iloc[:,1:]\n",
    "test = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_2_200Samples.csv')).iloc[:,1:]\n",
    "true = np.array(test['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyper_Parameters = DOE_Hyper()\n",
    "mean_abs_error = np.zeros(Hyper_Parameters.shape[0])\n",
    "for i in range(Hyper_Parameters.shape[0]):\n",
    "    temp = np.array(Hyper_Parameters.iloc[i,:])\n",
    "    model,pred_k_m = _SVR(train,test.iloc[:,:-1],temp)\n",
    "    mean_abs_error[i] = np.mean((abs(true-pred_k_m) / abs(true) ) * 100)\n",
    "Hyper_Parameters ['Mean_Error'] = pd.Series(mean_abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gamma               37.213462\n",
       "Regularization    1000.000000\n",
       "Name: 92, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hyper_Parameters.iloc[Hyper_Parameters.Mean_Error.idxmin(),:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
