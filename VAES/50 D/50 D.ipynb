{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.3.0+cpu cuda: False\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.utils.data\n",
    "import torch.nn.init as init\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import VAE\n",
    "\n",
    "print( 'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Re-Scale Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"train_2_1000Samples.csv\"\n",
    "train_data = pd.read_csv(path).iloc[:,1:]\n",
    "test_data = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_2_200Samples.csv')).iloc[:,1:]\n",
    "train_data = train_data.iloc[:,:-1]\n",
    "test_data = test_data.iloc[:,:-1]\n",
    "train_data.columns  = test_data.columns\n",
    "cols = test_data.columns\n",
    "scalar = MinMaxScaler().fit(pd.concat([train_data, test_data]))\n",
    "train_data = pd.DataFrame(scalar.transform (train_data)) \n",
    "test_data = pd.DataFrame(scalar.transform (test_data)) \n",
    "train_data.columns = cols\n",
    "test_data.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the Global Variables for Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "x_dim = train_data.shape[1]\n",
    "h_dim = 250\n",
    "z_dim = 25\n",
    "n_epochs = 40\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "seed = 100\n",
    "print_every = 10\n",
    "save_every = 10\n",
    "#manual seed\n",
    "torch.manual_seed(seed)\n",
    "#init model + optimizer + datasets\n",
    "train_loader = torch.utils.data.DataLoader ( dataset = train_data.values ,  batch_size = batch_size , shuffle= True)\n",
    "test_loader = torch.utils.data.DataLoader (  dataset = test_data.values , shuffle= True)\n",
    "model = VAE.VAE(x_dim, h_dim, z_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    epoch_loss = np.zeros(int(len (train_data) / batch_size ))\n",
    "    epoch_div = np.zeros(int(len (train_data) / batch_size))\n",
    "    for batch_idx, (data) in enumerate(train_loader):\n",
    "        \n",
    "        data = Variable(data)\n",
    "        #forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        kld_loss, nll_loss, lat, recon, _ = model(data)\n",
    "        epoch_loss [batch_idx] = nll_loss\n",
    "        epoch_div [batch_idx] = kld_loss\n",
    "        loss = kld_loss + nll_loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        #printing\n",
    "        if batch_idx % print_every == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t KLD Loss: {:.6f} \\t NLL Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                kld_loss.data / batch_size,\n",
    "                nll_loss.data / batch_size))\n",
    "\n",
    "            \n",
    "\n",
    "        train_loss += loss.data\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "    return epoch_loss, epoch_div\n",
    "    \n",
    "def test(epoch):\n",
    "    \"\"\"uses test data to evaluate \n",
    "    likelihood of the model\"\"\"\n",
    "    mean_kld_loss, mean_nll_loss = 0, 0\n",
    "    epoch_loss = np.zeros(len(test_data))\n",
    "    epoch_div = np.zeros(len(test_data))\n",
    "    for i, (data) in enumerate(test_loader):                                           \n",
    "        \n",
    "        data = Variable(data.reshape(1,-1))\n",
    "        kld_loss, nll_loss, _, _, _ = model(data)\n",
    "        epoch_div [i] = kld_loss\n",
    "        epoch_loss [i] = nll_loss\n",
    "        mean_kld_loss += kld_loss.data\n",
    "        mean_nll_loss += nll_loss.data\n",
    "\n",
    "    mean_kld_loss /= len(test_loader.dataset)\n",
    "    mean_nll_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('====> Test set loss: KLD Loss = {:.4f}, NLL Loss = {:.4f} '.format(\n",
    "        mean_kld_loss, mean_nll_loss))\n",
    "    return epoch_loss, epoch_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1000 (0%)]\t KLD Loss: 9.617811 \t NLL Loss: 56.044445\n",
      "Train Epoch: 1 [100/1000 (10%)]\t KLD Loss: 7.006464 \t NLL Loss: 36.980578\n",
      "Train Epoch: 1 [200/1000 (20%)]\t KLD Loss: 5.451535 \t NLL Loss: 22.454949\n",
      "Train Epoch: 1 [300/1000 (30%)]\t KLD Loss: 5.558869 \t NLL Loss: 10.776991\n",
      "Train Epoch: 1 [400/1000 (40%)]\t KLD Loss: 3.436667 \t NLL Loss: 9.738281\n",
      "Train Epoch: 1 [500/1000 (50%)]\t KLD Loss: 2.004300 \t NLL Loss: 10.339705\n",
      "Train Epoch: 1 [600/1000 (60%)]\t KLD Loss: 1.817669 \t NLL Loss: 9.938670\n",
      "Train Epoch: 1 [700/1000 (70%)]\t KLD Loss: 1.626092 \t NLL Loss: 6.265789\n",
      "Train Epoch: 1 [800/1000 (80%)]\t KLD Loss: 1.740163 \t NLL Loss: 10.273572\n",
      "Train Epoch: 1 [900/1000 (90%)]\t KLD Loss: 1.198061 \t NLL Loss: 10.017974\n",
      "====> Epoch: 1 Average loss: 19.9136\n",
      "====> Test set loss: KLD Loss = 1.3069, NLL Loss = 8.9413 \n",
      "Train Epoch: 2 [0/1000 (0%)]\t KLD Loss: 1.330577 \t NLL Loss: 9.453414\n",
      "Train Epoch: 2 [100/1000 (10%)]\t KLD Loss: 1.092769 \t NLL Loss: 8.010003\n",
      "Train Epoch: 2 [200/1000 (20%)]\t KLD Loss: 1.152994 \t NLL Loss: 8.618243\n",
      "Train Epoch: 2 [300/1000 (30%)]\t KLD Loss: 1.015760 \t NLL Loss: 9.092271\n",
      "Train Epoch: 2 [400/1000 (40%)]\t KLD Loss: 0.894070 \t NLL Loss: 8.236453\n",
      "Train Epoch: 2 [500/1000 (50%)]\t KLD Loss: 1.169062 \t NLL Loss: 6.590381\n",
      "Train Epoch: 2 [600/1000 (60%)]\t KLD Loss: 1.000708 \t NLL Loss: 6.227859\n",
      "Train Epoch: 2 [700/1000 (70%)]\t KLD Loss: 0.853815 \t NLL Loss: 7.652191\n",
      "Train Epoch: 2 [800/1000 (80%)]\t KLD Loss: 1.047569 \t NLL Loss: 3.419218\n",
      "Train Epoch: 2 [900/1000 (90%)]\t KLD Loss: 0.923215 \t NLL Loss: 2.910096\n",
      "====> Epoch: 2 Average loss: 7.4871\n",
      "====> Test set loss: KLD Loss = 0.9885, NLL Loss = 3.9383 \n",
      "Train Epoch: 3 [0/1000 (0%)]\t KLD Loss: 1.032644 \t NLL Loss: 3.754385\n",
      "Train Epoch: 3 [100/1000 (10%)]\t KLD Loss: 1.147605 \t NLL Loss: 2.185695\n",
      "Train Epoch: 3 [200/1000 (20%)]\t KLD Loss: 1.201438 \t NLL Loss: 2.914451\n",
      "Train Epoch: 3 [300/1000 (30%)]\t KLD Loss: 1.109320 \t NLL Loss: 2.652606\n",
      "Train Epoch: 3 [400/1000 (40%)]\t KLD Loss: 0.867208 \t NLL Loss: 1.075578\n",
      "Train Epoch: 3 [500/1000 (50%)]\t KLD Loss: 1.129160 \t NLL Loss: -0.744877\n",
      "Train Epoch: 3 [600/1000 (60%)]\t KLD Loss: 1.405768 \t NLL Loss: 0.312387\n",
      "Train Epoch: 3 [700/1000 (70%)]\t KLD Loss: 1.018713 \t NLL Loss: 1.604416\n",
      "Train Epoch: 3 [800/1000 (80%)]\t KLD Loss: 1.146554 \t NLL Loss: -1.920529\n",
      "Train Epoch: 3 [900/1000 (90%)]\t KLD Loss: 1.121831 \t NLL Loss: 0.404627\n",
      "====> Epoch: 3 Average loss: 1.8163\n",
      "====> Test set loss: KLD Loss = 1.1866, NLL Loss = -1.3112 \n",
      "Train Epoch: 4 [0/1000 (0%)]\t KLD Loss: 1.265504 \t NLL Loss: -3.405854\n",
      "Train Epoch: 4 [100/1000 (10%)]\t KLD Loss: 1.083018 \t NLL Loss: -0.293665\n",
      "Train Epoch: 4 [200/1000 (20%)]\t KLD Loss: 1.084686 \t NLL Loss: -3.583860\n",
      "Train Epoch: 4 [300/1000 (30%)]\t KLD Loss: 1.190023 \t NLL Loss: -5.935643\n",
      "Train Epoch: 4 [400/1000 (40%)]\t KLD Loss: 1.321056 \t NLL Loss: -3.311283\n",
      "Train Epoch: 4 [500/1000 (50%)]\t KLD Loss: 1.189907 \t NLL Loss: -4.343289\n",
      "Train Epoch: 4 [600/1000 (60%)]\t KLD Loss: 1.165059 \t NLL Loss: -5.720289\n",
      "Train Epoch: 4 [700/1000 (70%)]\t KLD Loss: 1.304790 \t NLL Loss: -4.572603\n",
      "Train Epoch: 4 [800/1000 (80%)]\t KLD Loss: 1.144809 \t NLL Loss: -4.669192\n",
      "Train Epoch: 4 [900/1000 (90%)]\t KLD Loss: 1.089278 \t NLL Loss: -4.661593\n",
      "====> Epoch: 4 Average loss: -3.6000\n",
      "====> Test set loss: KLD Loss = 1.1380, NLL Loss = -4.9445 \n",
      "Train Epoch: 5 [0/1000 (0%)]\t KLD Loss: 1.121587 \t NLL Loss: -6.410654\n",
      "Train Epoch: 5 [100/1000 (10%)]\t KLD Loss: 1.187759 \t NLL Loss: -5.136498\n",
      "Train Epoch: 5 [200/1000 (20%)]\t KLD Loss: 1.173387 \t NLL Loss: -9.900211\n",
      "Train Epoch: 5 [300/1000 (30%)]\t KLD Loss: 1.227798 \t NLL Loss: -8.865574\n",
      "Train Epoch: 5 [400/1000 (40%)]\t KLD Loss: 1.266246 \t NLL Loss: -6.495392\n",
      "Train Epoch: 5 [500/1000 (50%)]\t KLD Loss: 1.248004 \t NLL Loss: -8.435612\n",
      "Train Epoch: 5 [600/1000 (60%)]\t KLD Loss: 1.097446 \t NLL Loss: -9.052365\n",
      "Train Epoch: 5 [700/1000 (70%)]\t KLD Loss: 1.196487 \t NLL Loss: -11.157484\n",
      "Train Epoch: 5 [800/1000 (80%)]\t KLD Loss: 1.212015 \t NLL Loss: -12.376243\n",
      "Train Epoch: 5 [900/1000 (90%)]\t KLD Loss: 1.162295 \t NLL Loss: -11.634296\n",
      "====> Epoch: 5 Average loss: -8.4720\n",
      "====> Test set loss: KLD Loss = 1.2008, NLL Loss = -10.3660 \n",
      "Train Epoch: 6 [0/1000 (0%)]\t KLD Loss: 1.243496 \t NLL Loss: -12.913712\n",
      "Train Epoch: 6 [100/1000 (10%)]\t KLD Loss: 1.337551 \t NLL Loss: -13.989509\n",
      "Train Epoch: 6 [200/1000 (20%)]\t KLD Loss: 1.284598 \t NLL Loss: -12.509482\n",
      "Train Epoch: 6 [300/1000 (30%)]\t KLD Loss: 1.245829 \t NLL Loss: -13.529469\n",
      "Train Epoch: 6 [400/1000 (40%)]\t KLD Loss: 1.275667 \t NLL Loss: -11.323570\n",
      "Train Epoch: 6 [500/1000 (50%)]\t KLD Loss: 1.321597 \t NLL Loss: -12.483007\n",
      "Train Epoch: 6 [600/1000 (60%)]\t KLD Loss: 1.286792 \t NLL Loss: -17.620291\n",
      "Train Epoch: 6 [700/1000 (70%)]\t KLD Loss: 1.233949 \t NLL Loss: -16.152374\n",
      "Train Epoch: 6 [800/1000 (80%)]\t KLD Loss: 1.316013 \t NLL Loss: -15.484578\n",
      "Train Epoch: 6 [900/1000 (90%)]\t KLD Loss: 1.341364 \t NLL Loss: -12.512382\n",
      "====> Epoch: 6 Average loss: -12.5371\n",
      "====> Test set loss: KLD Loss = 1.2502, NLL Loss = -11.9232 \n",
      "Train Epoch: 7 [0/1000 (0%)]\t KLD Loss: 1.377587 \t NLL Loss: -17.481120\n",
      "Train Epoch: 7 [100/1000 (10%)]\t KLD Loss: 1.195989 \t NLL Loss: -16.018555\n",
      "Train Epoch: 7 [200/1000 (20%)]\t KLD Loss: 1.275317 \t NLL Loss: -15.669378\n",
      "Train Epoch: 7 [300/1000 (30%)]\t KLD Loss: 1.194595 \t NLL Loss: -16.480496\n",
      "Train Epoch: 7 [400/1000 (40%)]\t KLD Loss: 1.259696 \t NLL Loss: -17.827517\n",
      "Train Epoch: 7 [500/1000 (50%)]\t KLD Loss: 1.331040 \t NLL Loss: -16.218285\n",
      "Train Epoch: 7 [600/1000 (60%)]\t KLD Loss: 1.297301 \t NLL Loss: -16.254510\n",
      "Train Epoch: 7 [700/1000 (70%)]\t KLD Loss: 1.324690 \t NLL Loss: -16.881220\n",
      "Train Epoch: 7 [800/1000 (80%)]\t KLD Loss: 1.235272 \t NLL Loss: -17.071452\n",
      "Train Epoch: 7 [900/1000 (90%)]\t KLD Loss: 1.201542 \t NLL Loss: -15.342858\n",
      "====> Epoch: 7 Average loss: -14.9045\n",
      "====> Test set loss: KLD Loss = 1.2856, NLL Loss = -13.9402 \n",
      "Train Epoch: 8 [0/1000 (0%)]\t KLD Loss: 1.316034 \t NLL Loss: -16.486777\n",
      "Train Epoch: 8 [100/1000 (10%)]\t KLD Loss: 1.301931 \t NLL Loss: -19.925819\n",
      "Train Epoch: 8 [200/1000 (20%)]\t KLD Loss: 1.292881 \t NLL Loss: -18.227679\n",
      "Train Epoch: 8 [300/1000 (30%)]\t KLD Loss: 1.343948 \t NLL Loss: -21.218142\n",
      "Train Epoch: 8 [400/1000 (40%)]\t KLD Loss: 1.248776 \t NLL Loss: -18.003104\n",
      "Train Epoch: 8 [500/1000 (50%)]\t KLD Loss: 1.238873 \t NLL Loss: -14.523374\n",
      "Train Epoch: 8 [600/1000 (60%)]\t KLD Loss: 1.150815 \t NLL Loss: -21.884136\n",
      "Train Epoch: 8 [700/1000 (70%)]\t KLD Loss: 1.288404 \t NLL Loss: -15.906379\n",
      "Train Epoch: 8 [800/1000 (80%)]\t KLD Loss: 1.282606 \t NLL Loss: -17.067102\n",
      "Train Epoch: 8 [900/1000 (90%)]\t KLD Loss: 1.320392 \t NLL Loss: -18.240218\n",
      "====> Epoch: 8 Average loss: -17.5535\n",
      "====> Test set loss: KLD Loss = 1.2601, NLL Loss = -17.1098 \n",
      "Train Epoch: 9 [0/1000 (0%)]\t KLD Loss: 1.263955 \t NLL Loss: -18.750832\n",
      "Train Epoch: 9 [100/1000 (10%)]\t KLD Loss: 1.337302 \t NLL Loss: -19.395961\n",
      "Train Epoch: 9 [200/1000 (20%)]\t KLD Loss: 1.284624 \t NLL Loss: -17.313704\n",
      "Train Epoch: 9 [300/1000 (30%)]\t KLD Loss: 1.269611 \t NLL Loss: -19.682525\n",
      "Train Epoch: 9 [400/1000 (40%)]\t KLD Loss: 1.347991 \t NLL Loss: -21.860099\n",
      "Train Epoch: 9 [500/1000 (50%)]\t KLD Loss: 1.445199 \t NLL Loss: -19.126491\n",
      "Train Epoch: 9 [600/1000 (60%)]\t KLD Loss: 1.338146 \t NLL Loss: -18.278000\n",
      "Train Epoch: 9 [700/1000 (70%)]\t KLD Loss: 1.322012 \t NLL Loss: -20.608580\n",
      "Train Epoch: 9 [800/1000 (80%)]\t KLD Loss: 1.362468 \t NLL Loss: -20.499796\n",
      "Train Epoch: 9 [900/1000 (90%)]\t KLD Loss: 1.382845 \t NLL Loss: -16.180757\n",
      "====> Epoch: 9 Average loss: -18.7450\n",
      "====> Test set loss: KLD Loss = 1.2844, NLL Loss = -18.9730 \n",
      "Train Epoch: 10 [0/1000 (0%)]\t KLD Loss: 1.375611 \t NLL Loss: -23.437311\n",
      "Train Epoch: 10 [100/1000 (10%)]\t KLD Loss: 1.349298 \t NLL Loss: -20.486871\n",
      "Train Epoch: 10 [200/1000 (20%)]\t KLD Loss: 1.301513 \t NLL Loss: -20.341965\n",
      "Train Epoch: 10 [300/1000 (30%)]\t KLD Loss: 1.410532 \t NLL Loss: -23.537629\n",
      "Train Epoch: 10 [400/1000 (40%)]\t KLD Loss: 1.366975 \t NLL Loss: -16.348023\n",
      "Train Epoch: 10 [500/1000 (50%)]\t KLD Loss: 1.217192 \t NLL Loss: -19.895585\n",
      "Train Epoch: 10 [600/1000 (60%)]\t KLD Loss: 1.349523 \t NLL Loss: -12.080186\n",
      "Train Epoch: 10 [700/1000 (70%)]\t KLD Loss: 1.169691 \t NLL Loss: -18.772458\n",
      "Train Epoch: 10 [800/1000 (80%)]\t KLD Loss: 1.217563 \t NLL Loss: -21.233306\n",
      "Train Epoch: 10 [900/1000 (90%)]\t KLD Loss: 1.253705 \t NLL Loss: -26.168052\n",
      "====> Epoch: 10 Average loss: -19.4608\n",
      "====> Test set loss: KLD Loss = 1.3105, NLL Loss = -20.6077 \n",
      "Train Epoch: 11 [0/1000 (0%)]\t KLD Loss: 1.321380 \t NLL Loss: -22.988732\n",
      "Train Epoch: 11 [100/1000 (10%)]\t KLD Loss: 1.279761 \t NLL Loss: -24.609584\n",
      "Train Epoch: 11 [200/1000 (20%)]\t KLD Loss: 1.332983 \t NLL Loss: -23.123583\n",
      "Train Epoch: 11 [300/1000 (30%)]\t KLD Loss: 1.282741 \t NLL Loss: -23.842047\n",
      "Train Epoch: 11 [400/1000 (40%)]\t KLD Loss: 1.286019 \t NLL Loss: -26.318084\n",
      "Train Epoch: 11 [500/1000 (50%)]\t KLD Loss: 1.214802 \t NLL Loss: -16.331945\n",
      "Train Epoch: 11 [600/1000 (60%)]\t KLD Loss: 1.286370 \t NLL Loss: -20.325665\n",
      "Train Epoch: 11 [700/1000 (70%)]\t KLD Loss: 1.379540 \t NLL Loss: -25.738690\n",
      "Train Epoch: 11 [800/1000 (80%)]\t KLD Loss: 1.309998 \t NLL Loss: -24.970752\n",
      "Train Epoch: 11 [900/1000 (90%)]\t KLD Loss: 1.332860 \t NLL Loss: -24.340691\n",
      "====> Epoch: 11 Average loss: -22.0826\n",
      "====> Test set loss: KLD Loss = 1.3131, NLL Loss = -21.2576 \n",
      "Train Epoch: 12 [0/1000 (0%)]\t KLD Loss: 1.379238 \t NLL Loss: -25.915602\n",
      "Train Epoch: 12 [100/1000 (10%)]\t KLD Loss: 1.300505 \t NLL Loss: -22.623233\n",
      "Train Epoch: 12 [200/1000 (20%)]\t KLD Loss: 1.327625 \t NLL Loss: -23.874134\n",
      "Train Epoch: 12 [300/1000 (30%)]\t KLD Loss: 1.337653 \t NLL Loss: -18.871344\n",
      "Train Epoch: 12 [400/1000 (40%)]\t KLD Loss: 1.190852 \t NLL Loss: -24.027754\n",
      "Train Epoch: 12 [500/1000 (50%)]\t KLD Loss: 1.365626 \t NLL Loss: -26.242575\n",
      "Train Epoch: 12 [600/1000 (60%)]\t KLD Loss: 1.355753 \t NLL Loss: -20.795716\n",
      "Train Epoch: 12 [700/1000 (70%)]\t KLD Loss: 1.311269 \t NLL Loss: -26.438202\n",
      "Train Epoch: 12 [800/1000 (80%)]\t KLD Loss: 1.239971 \t NLL Loss: -25.780597\n",
      "Train Epoch: 12 [900/1000 (90%)]\t KLD Loss: 1.212613 \t NLL Loss: -24.357019\n",
      "====> Epoch: 12 Average loss: -22.3272\n",
      "====> Test set loss: KLD Loss = 1.2968, NLL Loss = -19.4962 \n",
      "Train Epoch: 13 [0/1000 (0%)]\t KLD Loss: 1.417142 \t NLL Loss: -21.869757\n",
      "Train Epoch: 13 [100/1000 (10%)]\t KLD Loss: 1.238029 \t NLL Loss: -25.016284\n",
      "Train Epoch: 13 [200/1000 (20%)]\t KLD Loss: 1.303296 \t NLL Loss: -24.503168\n",
      "Train Epoch: 13 [300/1000 (30%)]\t KLD Loss: 1.296635 \t NLL Loss: -24.598894\n",
      "Train Epoch: 13 [400/1000 (40%)]\t KLD Loss: 1.383301 \t NLL Loss: -23.204894\n",
      "Train Epoch: 13 [500/1000 (50%)]\t KLD Loss: 1.335961 \t NLL Loss: -25.580689\n",
      "Train Epoch: 13 [600/1000 (60%)]\t KLD Loss: 1.255919 \t NLL Loss: -24.996906\n",
      "Train Epoch: 13 [700/1000 (70%)]\t KLD Loss: 1.289589 \t NLL Loss: -26.501409\n",
      "Train Epoch: 13 [800/1000 (80%)]\t KLD Loss: 1.396821 \t NLL Loss: -27.394308\n",
      "Train Epoch: 13 [900/1000 (90%)]\t KLD Loss: 1.352099 \t NLL Loss: -24.691151\n",
      "====> Epoch: 13 Average loss: -24.3023\n",
      "====> Test set loss: KLD Loss = 1.3507, NLL Loss = -22.2631 \n",
      "Train Epoch: 14 [0/1000 (0%)]\t KLD Loss: 1.408480 \t NLL Loss: -23.840358\n",
      "Train Epoch: 14 [100/1000 (10%)]\t KLD Loss: 1.435634 \t NLL Loss: -26.527804\n",
      "Train Epoch: 14 [200/1000 (20%)]\t KLD Loss: 1.385827 \t NLL Loss: -27.235446\n",
      "Train Epoch: 14 [300/1000 (30%)]\t KLD Loss: 1.296503 \t NLL Loss: -26.370660\n",
      "Train Epoch: 14 [400/1000 (40%)]\t KLD Loss: 1.395305 \t NLL Loss: -23.938428\n",
      "Train Epoch: 14 [500/1000 (50%)]\t KLD Loss: 1.345078 \t NLL Loss: -24.215874\n",
      "Train Epoch: 14 [600/1000 (60%)]\t KLD Loss: 1.386198 \t NLL Loss: -23.763425\n",
      "Train Epoch: 14 [700/1000 (70%)]\t KLD Loss: 1.358416 \t NLL Loss: -27.112409\n",
      "Train Epoch: 14 [800/1000 (80%)]\t KLD Loss: 1.380559 \t NLL Loss: -28.119809\n",
      "Train Epoch: 14 [900/1000 (90%)]\t KLD Loss: 1.364262 \t NLL Loss: -26.200722\n",
      "====> Epoch: 14 Average loss: -26.1184\n",
      "====> Test set loss: KLD Loss = 1.3147, NLL Loss = -22.1093 \n",
      "Train Epoch: 15 [0/1000 (0%)]\t KLD Loss: 1.323147 \t NLL Loss: -24.544239\n",
      "Train Epoch: 15 [100/1000 (10%)]\t KLD Loss: 1.292676 \t NLL Loss: -24.811229\n",
      "Train Epoch: 15 [200/1000 (20%)]\t KLD Loss: 1.421551 \t NLL Loss: -25.846091\n",
      "Train Epoch: 15 [300/1000 (30%)]\t KLD Loss: 1.300228 \t NLL Loss: -25.451527\n",
      "Train Epoch: 15 [400/1000 (40%)]\t KLD Loss: 1.405780 \t NLL Loss: -27.523922\n",
      "Train Epoch: 15 [500/1000 (50%)]\t KLD Loss: 1.253006 \t NLL Loss: -29.324331\n",
      "Train Epoch: 15 [600/1000 (60%)]\t KLD Loss: 1.255727 \t NLL Loss: -26.787244\n",
      "Train Epoch: 15 [700/1000 (70%)]\t KLD Loss: 1.257460 \t NLL Loss: -24.459458\n",
      "Train Epoch: 15 [800/1000 (80%)]\t KLD Loss: 1.228243 \t NLL Loss: -30.181231\n",
      "Train Epoch: 15 [900/1000 (90%)]\t KLD Loss: 1.219754 \t NLL Loss: -28.072092\n",
      "====> Epoch: 15 Average loss: -26.5989\n",
      "====> Test set loss: KLD Loss = 1.3090, NLL Loss = -26.5849 \n",
      "Train Epoch: 16 [0/1000 (0%)]\t KLD Loss: 1.358793 \t NLL Loss: -27.922274\n",
      "Train Epoch: 16 [100/1000 (10%)]\t KLD Loss: 1.227844 \t NLL Loss: -28.449166\n",
      "Train Epoch: 16 [200/1000 (20%)]\t KLD Loss: 1.279760 \t NLL Loss: -29.382595\n",
      "Train Epoch: 16 [300/1000 (30%)]\t KLD Loss: 1.312362 \t NLL Loss: -25.588266\n",
      "Train Epoch: 16 [400/1000 (40%)]\t KLD Loss: 1.309239 \t NLL Loss: -27.409504\n",
      "Train Epoch: 16 [500/1000 (50%)]\t KLD Loss: 1.461859 \t NLL Loss: -27.115296\n",
      "Train Epoch: 16 [600/1000 (60%)]\t KLD Loss: 1.298174 \t NLL Loss: -32.429255\n",
      "Train Epoch: 16 [700/1000 (70%)]\t KLD Loss: 1.350409 \t NLL Loss: -30.099144\n",
      "Train Epoch: 16 [800/1000 (80%)]\t KLD Loss: 1.340460 \t NLL Loss: -29.356724\n",
      "Train Epoch: 16 [900/1000 (90%)]\t KLD Loss: 1.266812 \t NLL Loss: -29.365565\n",
      "====> Epoch: 16 Average loss: -28.7962\n",
      "====> Test set loss: KLD Loss = 1.3038, NLL Loss = -28.8109 \n",
      "Train Epoch: 17 [0/1000 (0%)]\t KLD Loss: 1.465742 \t NLL Loss: -31.606489\n",
      "Train Epoch: 17 [100/1000 (10%)]\t KLD Loss: 1.396677 \t NLL Loss: -29.709940\n",
      "Train Epoch: 17 [200/1000 (20%)]\t KLD Loss: 1.241530 \t NLL Loss: -29.907956\n",
      "Train Epoch: 17 [300/1000 (30%)]\t KLD Loss: 1.318935 \t NLL Loss: -28.704001\n",
      "Train Epoch: 17 [400/1000 (40%)]\t KLD Loss: 1.287013 \t NLL Loss: -30.810268\n",
      "Train Epoch: 17 [500/1000 (50%)]\t KLD Loss: 1.270281 \t NLL Loss: -30.718734\n",
      "Train Epoch: 17 [600/1000 (60%)]\t KLD Loss: 1.356046 \t NLL Loss: -33.581183\n",
      "Train Epoch: 17 [700/1000 (70%)]\t KLD Loss: 1.411959 \t NLL Loss: -31.038188\n",
      "Train Epoch: 17 [800/1000 (80%)]\t KLD Loss: 1.367544 \t NLL Loss: -34.420401\n",
      "Train Epoch: 17 [900/1000 (90%)]\t KLD Loss: 1.264155 \t NLL Loss: -35.842460\n",
      "====> Epoch: 17 Average loss: -29.8406\n",
      "====> Test set loss: KLD Loss = 1.2909, NLL Loss = -28.6599 \n",
      "Train Epoch: 18 [0/1000 (0%)]\t KLD Loss: 1.304542 \t NLL Loss: -28.475831\n",
      "Train Epoch: 18 [100/1000 (10%)]\t KLD Loss: 1.314226 \t NLL Loss: -29.947143\n",
      "Train Epoch: 18 [200/1000 (20%)]\t KLD Loss: 1.331429 \t NLL Loss: -32.372189\n",
      "Train Epoch: 18 [300/1000 (30%)]\t KLD Loss: 1.240886 \t NLL Loss: -29.643036\n",
      "Train Epoch: 18 [400/1000 (40%)]\t KLD Loss: 1.260991 \t NLL Loss: -33.950978\n",
      "Train Epoch: 18 [500/1000 (50%)]\t KLD Loss: 1.214750 \t NLL Loss: -32.000699\n",
      "Train Epoch: 18 [600/1000 (60%)]\t KLD Loss: 1.373325 \t NLL Loss: -31.574825\n",
      "Train Epoch: 18 [700/1000 (70%)]\t KLD Loss: 1.239262 \t NLL Loss: -30.974617\n",
      "Train Epoch: 18 [800/1000 (80%)]\t KLD Loss: 1.274226 \t NLL Loss: -31.358856\n",
      "Train Epoch: 18 [900/1000 (90%)]\t KLD Loss: 1.297101 \t NLL Loss: -32.014405\n",
      "====> Epoch: 18 Average loss: -30.0432\n",
      "====> Test set loss: KLD Loss = 1.3325, NLL Loss = -29.5973 \n",
      "Train Epoch: 19 [0/1000 (0%)]\t KLD Loss: 1.305699 \t NLL Loss: -29.985395\n",
      "Train Epoch: 19 [100/1000 (10%)]\t KLD Loss: 1.368754 \t NLL Loss: -31.317682\n",
      "Train Epoch: 19 [200/1000 (20%)]\t KLD Loss: 1.312712 \t NLL Loss: -28.882263\n",
      "Train Epoch: 19 [300/1000 (30%)]\t KLD Loss: 1.302494 \t NLL Loss: -31.539920\n",
      "Train Epoch: 19 [400/1000 (40%)]\t KLD Loss: 1.281153 \t NLL Loss: -30.594894\n",
      "Train Epoch: 19 [500/1000 (50%)]\t KLD Loss: 1.312609 \t NLL Loss: -33.821372\n",
      "Train Epoch: 19 [600/1000 (60%)]\t KLD Loss: 1.274736 \t NLL Loss: -34.858191\n",
      "Train Epoch: 19 [700/1000 (70%)]\t KLD Loss: 1.338271 \t NLL Loss: -34.761635\n",
      "Train Epoch: 19 [800/1000 (80%)]\t KLD Loss: 1.323732 \t NLL Loss: -31.363252\n",
      "Train Epoch: 19 [900/1000 (90%)]\t KLD Loss: 1.314738 \t NLL Loss: -38.461549\n",
      "====> Epoch: 19 Average loss: -31.4295\n",
      "====> Test set loss: KLD Loss = 1.3122, NLL Loss = -29.8790 \n",
      "Train Epoch: 20 [0/1000 (0%)]\t KLD Loss: 1.311489 \t NLL Loss: -34.359045\n",
      "Train Epoch: 20 [100/1000 (10%)]\t KLD Loss: 1.369401 \t NLL Loss: -32.791767\n",
      "Train Epoch: 20 [200/1000 (20%)]\t KLD Loss: 1.375845 \t NLL Loss: -35.948528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [300/1000 (30%)]\t KLD Loss: 1.245117 \t NLL Loss: -29.102649\n",
      "Train Epoch: 20 [400/1000 (40%)]\t KLD Loss: 1.431165 \t NLL Loss: -35.734675\n",
      "Train Epoch: 20 [500/1000 (50%)]\t KLD Loss: 1.350266 \t NLL Loss: -31.210643\n",
      "Train Epoch: 20 [600/1000 (60%)]\t KLD Loss: 1.372555 \t NLL Loss: -34.147420\n",
      "Train Epoch: 20 [700/1000 (70%)]\t KLD Loss: 1.365668 \t NLL Loss: -32.173623\n",
      "Train Epoch: 20 [800/1000 (80%)]\t KLD Loss: 1.363986 \t NLL Loss: -38.631681\n",
      "Train Epoch: 20 [900/1000 (90%)]\t KLD Loss: 1.294793 \t NLL Loss: -35.834687\n",
      "====> Epoch: 20 Average loss: -32.9350\n",
      "====> Test set loss: KLD Loss = 1.3555, NLL Loss = -33.3746 \n",
      "Train Epoch: 21 [0/1000 (0%)]\t KLD Loss: 1.335259 \t NLL Loss: -36.685435\n",
      "Train Epoch: 21 [100/1000 (10%)]\t KLD Loss: 1.261760 \t NLL Loss: -36.265507\n",
      "Train Epoch: 21 [200/1000 (20%)]\t KLD Loss: 1.268936 \t NLL Loss: -36.688162\n",
      "Train Epoch: 21 [300/1000 (30%)]\t KLD Loss: 1.327939 \t NLL Loss: -35.377469\n",
      "Train Epoch: 21 [400/1000 (40%)]\t KLD Loss: 1.359772 \t NLL Loss: -30.771575\n",
      "Train Epoch: 21 [500/1000 (50%)]\t KLD Loss: 1.436505 \t NLL Loss: -29.786848\n",
      "Train Epoch: 21 [600/1000 (60%)]\t KLD Loss: 1.395758 \t NLL Loss: -34.030036\n",
      "Train Epoch: 21 [700/1000 (70%)]\t KLD Loss: 1.345371 \t NLL Loss: -35.755756\n",
      "Train Epoch: 21 [800/1000 (80%)]\t KLD Loss: 1.392421 \t NLL Loss: -33.353020\n",
      "Train Epoch: 21 [900/1000 (90%)]\t KLD Loss: 1.347227 \t NLL Loss: -32.185351\n",
      "====> Epoch: 21 Average loss: -33.2625\n",
      "====> Test set loss: KLD Loss = 1.2863, NLL Loss = -32.8235 \n",
      "Train Epoch: 22 [0/1000 (0%)]\t KLD Loss: 1.138466 \t NLL Loss: -34.034988\n",
      "Train Epoch: 22 [100/1000 (10%)]\t KLD Loss: 1.299142 \t NLL Loss: -35.596196\n",
      "Train Epoch: 22 [200/1000 (20%)]\t KLD Loss: 1.379729 \t NLL Loss: -33.938421\n",
      "Train Epoch: 22 [300/1000 (30%)]\t KLD Loss: 1.352635 \t NLL Loss: -34.931342\n",
      "Train Epoch: 22 [400/1000 (40%)]\t KLD Loss: 1.247259 \t NLL Loss: -33.876231\n",
      "Train Epoch: 22 [500/1000 (50%)]\t KLD Loss: 1.258785 \t NLL Loss: -36.395066\n",
      "Train Epoch: 22 [600/1000 (60%)]\t KLD Loss: 1.286495 \t NLL Loss: -37.038343\n",
      "Train Epoch: 22 [700/1000 (70%)]\t KLD Loss: 1.395231 \t NLL Loss: -37.512986\n",
      "Train Epoch: 22 [800/1000 (80%)]\t KLD Loss: 1.229068 \t NLL Loss: -33.292934\n",
      "Train Epoch: 22 [900/1000 (90%)]\t KLD Loss: 1.318603 \t NLL Loss: -35.088645\n",
      "====> Epoch: 22 Average loss: -34.8845\n",
      "====> Test set loss: KLD Loss = 1.3321, NLL Loss = -36.9072 \n",
      "Train Epoch: 23 [0/1000 (0%)]\t KLD Loss: 1.363792 \t NLL Loss: -38.327555\n",
      "Train Epoch: 23 [100/1000 (10%)]\t KLD Loss: 1.424782 \t NLL Loss: -33.131433\n",
      "Train Epoch: 23 [200/1000 (20%)]\t KLD Loss: 1.401995 \t NLL Loss: -39.325951\n",
      "Train Epoch: 23 [300/1000 (30%)]\t KLD Loss: 1.449143 \t NLL Loss: -31.945689\n",
      "Train Epoch: 23 [400/1000 (40%)]\t KLD Loss: 1.397148 \t NLL Loss: -33.880376\n",
      "Train Epoch: 23 [500/1000 (50%)]\t KLD Loss: 1.321748 \t NLL Loss: -37.426473\n",
      "Train Epoch: 23 [600/1000 (60%)]\t KLD Loss: 1.206102 \t NLL Loss: -36.726686\n",
      "Train Epoch: 23 [700/1000 (70%)]\t KLD Loss: 1.327426 \t NLL Loss: -27.015288\n",
      "Train Epoch: 23 [800/1000 (80%)]\t KLD Loss: 1.331730 \t NLL Loss: -38.881808\n",
      "Train Epoch: 23 [900/1000 (90%)]\t KLD Loss: 1.289710 \t NLL Loss: -33.196868\n",
      "====> Epoch: 23 Average loss: -34.9613\n",
      "====> Test set loss: KLD Loss = 1.3081, NLL Loss = -33.2274 \n",
      "Train Epoch: 24 [0/1000 (0%)]\t KLD Loss: 1.309175 \t NLL Loss: -33.542304\n",
      "Train Epoch: 24 [100/1000 (10%)]\t KLD Loss: 1.117620 \t NLL Loss: -36.432175\n",
      "Train Epoch: 24 [200/1000 (20%)]\t KLD Loss: 1.339224 \t NLL Loss: -38.342178\n",
      "Train Epoch: 24 [300/1000 (30%)]\t KLD Loss: 1.317956 \t NLL Loss: -36.074808\n",
      "Train Epoch: 24 [400/1000 (40%)]\t KLD Loss: 1.201735 \t NLL Loss: -35.945922\n",
      "Train Epoch: 24 [500/1000 (50%)]\t KLD Loss: 1.350512 \t NLL Loss: -39.166867\n",
      "Train Epoch: 24 [600/1000 (60%)]\t KLD Loss: 1.250137 \t NLL Loss: -33.899362\n",
      "Train Epoch: 24 [700/1000 (70%)]\t KLD Loss: 1.354606 \t NLL Loss: -39.626359\n",
      "Train Epoch: 24 [800/1000 (80%)]\t KLD Loss: 1.306098 \t NLL Loss: -35.117195\n",
      "Train Epoch: 24 [900/1000 (90%)]\t KLD Loss: 1.302750 \t NLL Loss: -36.527893\n",
      "====> Epoch: 24 Average loss: -35.0146\n",
      "====> Test set loss: KLD Loss = 1.3327, NLL Loss = -33.3541 \n",
      "Train Epoch: 25 [0/1000 (0%)]\t KLD Loss: 1.421515 \t NLL Loss: -37.368747\n",
      "Train Epoch: 25 [100/1000 (10%)]\t KLD Loss: 1.286259 \t NLL Loss: -38.701487\n",
      "Train Epoch: 25 [200/1000 (20%)]\t KLD Loss: 1.391911 \t NLL Loss: -34.897452\n",
      "Train Epoch: 25 [300/1000 (30%)]\t KLD Loss: 1.303881 \t NLL Loss: -37.751797\n",
      "Train Epoch: 25 [400/1000 (40%)]\t KLD Loss: 1.415520 \t NLL Loss: -36.226019\n",
      "Train Epoch: 25 [500/1000 (50%)]\t KLD Loss: 1.377486 \t NLL Loss: -37.333007\n",
      "Train Epoch: 25 [600/1000 (60%)]\t KLD Loss: 1.285123 \t NLL Loss: -40.607301\n",
      "Train Epoch: 25 [700/1000 (70%)]\t KLD Loss: 1.416534 \t NLL Loss: -33.911758\n",
      "Train Epoch: 25 [800/1000 (80%)]\t KLD Loss: 1.321071 \t NLL Loss: -39.900461\n",
      "Train Epoch: 25 [900/1000 (90%)]\t KLD Loss: 1.434106 \t NLL Loss: -35.823695\n",
      "====> Epoch: 25 Average loss: -36.1935\n",
      "====> Test set loss: KLD Loss = 1.3257, NLL Loss = -36.8693 \n",
      "Train Epoch: 26 [0/1000 (0%)]\t KLD Loss: 1.320290 \t NLL Loss: -38.962104\n",
      "Train Epoch: 26 [100/1000 (10%)]\t KLD Loss: 1.355288 \t NLL Loss: -40.053281\n",
      "Train Epoch: 26 [200/1000 (20%)]\t KLD Loss: 1.263071 \t NLL Loss: -39.767475\n",
      "Train Epoch: 26 [300/1000 (30%)]\t KLD Loss: 1.329213 \t NLL Loss: -34.464839\n",
      "Train Epoch: 26 [400/1000 (40%)]\t KLD Loss: 1.300746 \t NLL Loss: -37.796214\n",
      "Train Epoch: 26 [500/1000 (50%)]\t KLD Loss: 1.275172 \t NLL Loss: -33.109752\n",
      "Train Epoch: 26 [600/1000 (60%)]\t KLD Loss: 1.308042 \t NLL Loss: -36.489541\n",
      "Train Epoch: 26 [700/1000 (70%)]\t KLD Loss: 1.286144 \t NLL Loss: -40.405135\n",
      "Train Epoch: 26 [800/1000 (80%)]\t KLD Loss: 1.293917 \t NLL Loss: -35.132816\n",
      "Train Epoch: 26 [900/1000 (90%)]\t KLD Loss: 1.432231 \t NLL Loss: -42.232276\n",
      "====> Epoch: 26 Average loss: -36.2960\n",
      "====> Test set loss: KLD Loss = 1.2972, NLL Loss = -37.5001 \n",
      "Train Epoch: 27 [0/1000 (0%)]\t KLD Loss: 1.342356 \t NLL Loss: -37.384896\n",
      "Train Epoch: 27 [100/1000 (10%)]\t KLD Loss: 1.216342 \t NLL Loss: -41.565017\n",
      "Train Epoch: 27 [200/1000 (20%)]\t KLD Loss: 1.257628 \t NLL Loss: -37.844415\n",
      "Train Epoch: 27 [300/1000 (30%)]\t KLD Loss: 1.167040 \t NLL Loss: -38.750028\n",
      "Train Epoch: 27 [400/1000 (40%)]\t KLD Loss: 1.341035 \t NLL Loss: -38.972441\n",
      "Train Epoch: 27 [500/1000 (50%)]\t KLD Loss: 1.165746 \t NLL Loss: -38.097796\n",
      "Train Epoch: 27 [600/1000 (60%)]\t KLD Loss: 1.287503 \t NLL Loss: -37.510733\n",
      "Train Epoch: 27 [700/1000 (70%)]\t KLD Loss: 1.266387 \t NLL Loss: -39.649357\n",
      "Train Epoch: 27 [800/1000 (80%)]\t KLD Loss: 1.269547 \t NLL Loss: -32.214800\n",
      "Train Epoch: 27 [900/1000 (90%)]\t KLD Loss: 1.281194 \t NLL Loss: -37.917099\n",
      "====> Epoch: 27 Average loss: -37.4017\n",
      "====> Test set loss: KLD Loss = 1.2922, NLL Loss = -39.2005 \n",
      "Train Epoch: 28 [0/1000 (0%)]\t KLD Loss: 1.203074 \t NLL Loss: -40.391390\n",
      "Train Epoch: 28 [100/1000 (10%)]\t KLD Loss: 1.257120 \t NLL Loss: -37.453455\n",
      "Train Epoch: 28 [200/1000 (20%)]\t KLD Loss: 1.230738 \t NLL Loss: -41.339091\n",
      "Train Epoch: 28 [300/1000 (30%)]\t KLD Loss: 1.263946 \t NLL Loss: -41.062069\n",
      "Train Epoch: 28 [400/1000 (40%)]\t KLD Loss: 1.375903 \t NLL Loss: -35.333055\n",
      "Train Epoch: 28 [500/1000 (50%)]\t KLD Loss: 1.348522 \t NLL Loss: -39.504552\n",
      "Train Epoch: 28 [600/1000 (60%)]\t KLD Loss: 1.339768 \t NLL Loss: -39.433694\n",
      "Train Epoch: 28 [700/1000 (70%)]\t KLD Loss: 1.169832 \t NLL Loss: -35.596248\n",
      "Train Epoch: 28 [800/1000 (80%)]\t KLD Loss: 1.226791 \t NLL Loss: -37.118466\n",
      "Train Epoch: 28 [900/1000 (90%)]\t KLD Loss: 1.323735 \t NLL Loss: -42.671446\n",
      "====> Epoch: 28 Average loss: -38.0909\n",
      "====> Test set loss: KLD Loss = 1.2811, NLL Loss = -34.1815 \n",
      "Train Epoch: 29 [0/1000 (0%)]\t KLD Loss: 1.375280 \t NLL Loss: -36.000129\n",
      "Train Epoch: 29 [100/1000 (10%)]\t KLD Loss: 1.329876 \t NLL Loss: -38.144562\n",
      "Train Epoch: 29 [200/1000 (20%)]\t KLD Loss: 1.346920 \t NLL Loss: -36.021437\n",
      "Train Epoch: 29 [300/1000 (30%)]\t KLD Loss: 1.327837 \t NLL Loss: -32.479196\n",
      "Train Epoch: 29 [400/1000 (40%)]\t KLD Loss: 1.332831 \t NLL Loss: -37.239397\n",
      "Train Epoch: 29 [500/1000 (50%)]\t KLD Loss: 1.384775 \t NLL Loss: -40.065035\n",
      "Train Epoch: 29 [600/1000 (60%)]\t KLD Loss: 1.409537 \t NLL Loss: -42.633586\n",
      "Train Epoch: 29 [700/1000 (70%)]\t KLD Loss: 1.270552 \t NLL Loss: -39.881469\n",
      "Train Epoch: 29 [800/1000 (80%)]\t KLD Loss: 1.361839 \t NLL Loss: -41.708485\n",
      "Train Epoch: 29 [900/1000 (90%)]\t KLD Loss: 1.261739 \t NLL Loss: -37.536213\n",
      "====> Epoch: 29 Average loss: -38.2211\n",
      "====> Test set loss: KLD Loss = 1.2803, NLL Loss = -36.1868 \n",
      "Train Epoch: 30 [0/1000 (0%)]\t KLD Loss: 1.289828 \t NLL Loss: -38.395414\n",
      "Train Epoch: 30 [100/1000 (10%)]\t KLD Loss: 1.334955 \t NLL Loss: -39.392948\n",
      "Train Epoch: 30 [200/1000 (20%)]\t KLD Loss: 1.380782 \t NLL Loss: -38.976496\n",
      "Train Epoch: 30 [300/1000 (30%)]\t KLD Loss: 1.276384 \t NLL Loss: -43.431717\n",
      "Train Epoch: 30 [400/1000 (40%)]\t KLD Loss: 1.315880 \t NLL Loss: -40.252014\n",
      "Train Epoch: 30 [500/1000 (50%)]\t KLD Loss: 1.325362 \t NLL Loss: -38.380597\n",
      "Train Epoch: 30 [600/1000 (60%)]\t KLD Loss: 1.249443 \t NLL Loss: -40.675051\n",
      "Train Epoch: 30 [700/1000 (70%)]\t KLD Loss: 1.362486 \t NLL Loss: -38.616580\n",
      "Train Epoch: 30 [800/1000 (80%)]\t KLD Loss: 1.332720 \t NLL Loss: -36.962275\n",
      "Train Epoch: 30 [900/1000 (90%)]\t KLD Loss: 1.342198 \t NLL Loss: -43.421158\n",
      "====> Epoch: 30 Average loss: -38.4383\n",
      "====> Test set loss: KLD Loss = 1.2570, NLL Loss = -35.5789 \n",
      "Train Epoch: 31 [0/1000 (0%)]\t KLD Loss: 1.393812 \t NLL Loss: -37.105528\n",
      "Train Epoch: 31 [100/1000 (10%)]\t KLD Loss: 1.181051 \t NLL Loss: -41.557676\n",
      "Train Epoch: 31 [200/1000 (20%)]\t KLD Loss: 1.175374 \t NLL Loss: -44.907236\n",
      "Train Epoch: 31 [300/1000 (30%)]\t KLD Loss: 1.190566 \t NLL Loss: -47.238239\n",
      "Train Epoch: 31 [400/1000 (40%)]\t KLD Loss: 1.256520 \t NLL Loss: -37.293563\n",
      "Train Epoch: 31 [500/1000 (50%)]\t KLD Loss: 1.187189 \t NLL Loss: -37.914234\n",
      "Train Epoch: 31 [600/1000 (60%)]\t KLD Loss: 1.253565 \t NLL Loss: -31.376980\n",
      "Train Epoch: 31 [700/1000 (70%)]\t KLD Loss: 1.299932 \t NLL Loss: -36.435464\n",
      "Train Epoch: 31 [800/1000 (80%)]\t KLD Loss: 1.166957 \t NLL Loss: -39.303871\n",
      "Train Epoch: 31 [900/1000 (90%)]\t KLD Loss: 1.295647 \t NLL Loss: -35.126765\n",
      "====> Epoch: 31 Average loss: -37.5157\n",
      "====> Test set loss: KLD Loss = 1.2752, NLL Loss = -38.9352 \n",
      "Train Epoch: 32 [0/1000 (0%)]\t KLD Loss: 1.243303 \t NLL Loss: -40.410828\n",
      "Train Epoch: 32 [100/1000 (10%)]\t KLD Loss: 1.215191 \t NLL Loss: -38.789849\n",
      "Train Epoch: 32 [200/1000 (20%)]\t KLD Loss: 1.187002 \t NLL Loss: -37.919565\n",
      "Train Epoch: 32 [300/1000 (30%)]\t KLD Loss: 1.301855 \t NLL Loss: -41.167013\n",
      "Train Epoch: 32 [400/1000 (40%)]\t KLD Loss: 1.315706 \t NLL Loss: -33.832215\n",
      "Train Epoch: 32 [500/1000 (50%)]\t KLD Loss: 1.218066 \t NLL Loss: -38.177600\n",
      "Train Epoch: 32 [600/1000 (60%)]\t KLD Loss: 1.353021 \t NLL Loss: -36.115591\n",
      "Train Epoch: 32 [700/1000 (70%)]\t KLD Loss: 1.318989 \t NLL Loss: -43.654468\n",
      "Train Epoch: 32 [800/1000 (80%)]\t KLD Loss: 1.275019 \t NLL Loss: -41.576029\n",
      "Train Epoch: 32 [900/1000 (90%)]\t KLD Loss: 1.282197 \t NLL Loss: -39.546944\n",
      "====> Epoch: 32 Average loss: -38.5099\n",
      "====> Test set loss: KLD Loss = 1.2150, NLL Loss = -33.8622 \n",
      "Train Epoch: 33 [0/1000 (0%)]\t KLD Loss: 1.262141 \t NLL Loss: -35.929351\n",
      "Train Epoch: 33 [100/1000 (10%)]\t KLD Loss: 1.409567 \t NLL Loss: -36.696122\n",
      "Train Epoch: 33 [200/1000 (20%)]\t KLD Loss: 1.134153 \t NLL Loss: -35.164102\n",
      "Train Epoch: 33 [300/1000 (30%)]\t KLD Loss: 1.204068 \t NLL Loss: -35.511625\n",
      "Train Epoch: 33 [400/1000 (40%)]\t KLD Loss: 1.169522 \t NLL Loss: -39.638921\n",
      "Train Epoch: 33 [500/1000 (50%)]\t KLD Loss: 1.270330 \t NLL Loss: -41.852146\n",
      "Train Epoch: 33 [600/1000 (60%)]\t KLD Loss: 1.275328 \t NLL Loss: -40.111895\n",
      "Train Epoch: 33 [700/1000 (70%)]\t KLD Loss: 1.213649 \t NLL Loss: -35.142737\n",
      "Train Epoch: 33 [800/1000 (80%)]\t KLD Loss: 1.345340 \t NLL Loss: -37.956397\n",
      "Train Epoch: 33 [900/1000 (90%)]\t KLD Loss: 1.303377 \t NLL Loss: -42.783211\n",
      "====> Epoch: 33 Average loss: -38.3788\n",
      "====> Test set loss: KLD Loss = 1.2343, NLL Loss = -38.4488 \n",
      "Train Epoch: 34 [0/1000 (0%)]\t KLD Loss: 1.265526 \t NLL Loss: -41.257129\n",
      "Train Epoch: 34 [100/1000 (10%)]\t KLD Loss: 1.169555 \t NLL Loss: -41.449298\n",
      "Train Epoch: 34 [200/1000 (20%)]\t KLD Loss: 1.339559 \t NLL Loss: -36.973669\n",
      "Train Epoch: 34 [300/1000 (30%)]\t KLD Loss: 1.236127 \t NLL Loss: -38.030667\n",
      "Train Epoch: 34 [400/1000 (40%)]\t KLD Loss: 1.286635 \t NLL Loss: -40.173634\n",
      "Train Epoch: 34 [500/1000 (50%)]\t KLD Loss: 1.244426 \t NLL Loss: -35.002580\n",
      "Train Epoch: 34 [600/1000 (60%)]\t KLD Loss: 1.342703 \t NLL Loss: -41.256654\n",
      "Train Epoch: 34 [700/1000 (70%)]\t KLD Loss: 1.252937 \t NLL Loss: -43.146309\n",
      "Train Epoch: 34 [800/1000 (80%)]\t KLD Loss: 1.258504 \t NLL Loss: -42.522597\n",
      "Train Epoch: 34 [900/1000 (90%)]\t KLD Loss: 1.339526 \t NLL Loss: -38.457879\n",
      "====> Epoch: 34 Average loss: -38.7382\n",
      "====> Test set loss: KLD Loss = 1.2634, NLL Loss = -36.0815 \n",
      "Train Epoch: 35 [0/1000 (0%)]\t KLD Loss: 1.288561 \t NLL Loss: -36.327320\n",
      "Train Epoch: 35 [100/1000 (10%)]\t KLD Loss: 1.102810 \t NLL Loss: -39.355128\n",
      "Train Epoch: 35 [200/1000 (20%)]\t KLD Loss: 1.208387 \t NLL Loss: -40.505982\n",
      "Train Epoch: 35 [300/1000 (30%)]\t KLD Loss: 1.215882 \t NLL Loss: -40.665547\n",
      "Train Epoch: 35 [400/1000 (40%)]\t KLD Loss: 1.300226 \t NLL Loss: -42.125464\n",
      "Train Epoch: 35 [500/1000 (50%)]\t KLD Loss: 1.322112 \t NLL Loss: -42.258757\n",
      "Train Epoch: 35 [600/1000 (60%)]\t KLD Loss: 1.245282 \t NLL Loss: -36.900845\n",
      "Train Epoch: 35 [700/1000 (70%)]\t KLD Loss: 1.200285 \t NLL Loss: -38.793592\n",
      "Train Epoch: 35 [800/1000 (80%)]\t KLD Loss: 1.349994 \t NLL Loss: -41.054915\n",
      "Train Epoch: 35 [900/1000 (90%)]\t KLD Loss: 1.189090 \t NLL Loss: -40.978855\n",
      "====> Epoch: 35 Average loss: -38.6190\n",
      "====> Test set loss: KLD Loss = 1.2587, NLL Loss = -41.1540 \n",
      "Train Epoch: 36 [0/1000 (0%)]\t KLD Loss: 1.262766 \t NLL Loss: -42.425741\n",
      "Train Epoch: 36 [100/1000 (10%)]\t KLD Loss: 1.220142 \t NLL Loss: -40.649574\n",
      "Train Epoch: 36 [200/1000 (20%)]\t KLD Loss: 1.278097 \t NLL Loss: -44.324331\n",
      "Train Epoch: 36 [300/1000 (30%)]\t KLD Loss: 1.219533 \t NLL Loss: -41.879667\n",
      "Train Epoch: 36 [400/1000 (40%)]\t KLD Loss: 1.253311 \t NLL Loss: -41.450028\n",
      "Train Epoch: 36 [500/1000 (50%)]\t KLD Loss: 1.307232 \t NLL Loss: -42.793724\n",
      "Train Epoch: 36 [600/1000 (60%)]\t KLD Loss: 1.072508 \t NLL Loss: -41.258092\n",
      "Train Epoch: 36 [700/1000 (70%)]\t KLD Loss: 1.363550 \t NLL Loss: -40.278141\n",
      "Train Epoch: 36 [800/1000 (80%)]\t KLD Loss: 1.131948 \t NLL Loss: -34.709795\n",
      "Train Epoch: 36 [900/1000 (90%)]\t KLD Loss: 1.249981 \t NLL Loss: -42.389535\n",
      "====> Epoch: 36 Average loss: -40.2139\n",
      "====> Test set loss: KLD Loss = 1.2359, NLL Loss = -40.4481 \n",
      "Train Epoch: 37 [0/1000 (0%)]\t KLD Loss: 1.236889 \t NLL Loss: -43.812588\n",
      "Train Epoch: 37 [100/1000 (10%)]\t KLD Loss: 1.218518 \t NLL Loss: -43.040258\n",
      "Train Epoch: 37 [200/1000 (20%)]\t KLD Loss: 1.155628 \t NLL Loss: -42.983113\n",
      "Train Epoch: 37 [300/1000 (30%)]\t KLD Loss: 1.335524 \t NLL Loss: -43.681119\n",
      "Train Epoch: 37 [400/1000 (40%)]\t KLD Loss: 1.185798 \t NLL Loss: -44.094254\n",
      "Train Epoch: 37 [500/1000 (50%)]\t KLD Loss: 1.153540 \t NLL Loss: -30.048781\n",
      "Train Epoch: 37 [600/1000 (60%)]\t KLD Loss: 1.269397 \t NLL Loss: -37.853824\n",
      "Train Epoch: 37 [700/1000 (70%)]\t KLD Loss: 1.280412 \t NLL Loss: -39.871050\n",
      "Train Epoch: 37 [800/1000 (80%)]\t KLD Loss: 1.216155 \t NLL Loss: -41.116414\n",
      "Train Epoch: 37 [900/1000 (90%)]\t KLD Loss: 1.194945 \t NLL Loss: -42.552659\n",
      "====> Epoch: 37 Average loss: -39.4990\n",
      "====> Test set loss: KLD Loss = 1.2323, NLL Loss = -38.2838 \n",
      "Train Epoch: 38 [0/1000 (0%)]\t KLD Loss: 1.297189 \t NLL Loss: -39.343970\n",
      "Train Epoch: 38 [100/1000 (10%)]\t KLD Loss: 1.283638 \t NLL Loss: -37.149009\n",
      "Train Epoch: 38 [200/1000 (20%)]\t KLD Loss: 1.129626 \t NLL Loss: -41.226288\n",
      "Train Epoch: 38 [300/1000 (30%)]\t KLD Loss: 1.261933 \t NLL Loss: -42.746402\n",
      "Train Epoch: 38 [400/1000 (40%)]\t KLD Loss: 1.241692 \t NLL Loss: -48.353829\n",
      "Train Epoch: 38 [500/1000 (50%)]\t KLD Loss: 1.201632 \t NLL Loss: -40.220445\n",
      "Train Epoch: 38 [600/1000 (60%)]\t KLD Loss: 1.260408 \t NLL Loss: -42.870654\n",
      "Train Epoch: 38 [700/1000 (70%)]\t KLD Loss: 1.184788 \t NLL Loss: -41.865067\n",
      "Train Epoch: 38 [800/1000 (80%)]\t KLD Loss: 1.341356 \t NLL Loss: -44.296529\n",
      "Train Epoch: 38 [900/1000 (90%)]\t KLD Loss: 1.221880 \t NLL Loss: -42.563681\n",
      "====> Epoch: 38 Average loss: -40.2405\n",
      "====> Test set loss: KLD Loss = 1.2441, NLL Loss = -40.1285 \n",
      "Train Epoch: 39 [0/1000 (0%)]\t KLD Loss: 1.279016 \t NLL Loss: -41.949989\n",
      "Train Epoch: 39 [100/1000 (10%)]\t KLD Loss: 1.175297 \t NLL Loss: -44.354606\n",
      "Train Epoch: 39 [200/1000 (20%)]\t KLD Loss: 1.119919 \t NLL Loss: -47.106194\n",
      "Train Epoch: 39 [300/1000 (30%)]\t KLD Loss: 1.268002 \t NLL Loss: -41.133199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39 [400/1000 (40%)]\t KLD Loss: 1.278014 \t NLL Loss: -42.899150\n",
      "Train Epoch: 39 [500/1000 (50%)]\t KLD Loss: 1.257759 \t NLL Loss: -43.483424\n",
      "Train Epoch: 39 [600/1000 (60%)]\t KLD Loss: 1.184289 \t NLL Loss: -39.217758\n",
      "Train Epoch: 39 [700/1000 (70%)]\t KLD Loss: 1.225090 \t NLL Loss: -42.312406\n",
      "Train Epoch: 39 [800/1000 (80%)]\t KLD Loss: 1.164493 \t NLL Loss: -40.081457\n",
      "Train Epoch: 39 [900/1000 (90%)]\t KLD Loss: 1.159845 \t NLL Loss: -38.418432\n",
      "====> Epoch: 39 Average loss: -40.4139\n",
      "====> Test set loss: KLD Loss = 1.2250, NLL Loss = -40.2917 \n",
      "Train Epoch: 40 [0/1000 (0%)]\t KLD Loss: 1.181031 \t NLL Loss: -41.078247\n",
      "Train Epoch: 40 [100/1000 (10%)]\t KLD Loss: 1.290032 \t NLL Loss: -43.288922\n",
      "Train Epoch: 40 [200/1000 (20%)]\t KLD Loss: 1.255428 \t NLL Loss: -46.245809\n",
      "Train Epoch: 40 [300/1000 (30%)]\t KLD Loss: 1.291833 \t NLL Loss: -39.323837\n",
      "Train Epoch: 40 [400/1000 (40%)]\t KLD Loss: 1.233235 \t NLL Loss: -40.513603\n",
      "Train Epoch: 40 [500/1000 (50%)]\t KLD Loss: 1.272018 \t NLL Loss: -38.274714\n",
      "Train Epoch: 40 [600/1000 (60%)]\t KLD Loss: 1.154318 \t NLL Loss: -39.831975\n",
      "Train Epoch: 40 [700/1000 (70%)]\t KLD Loss: 1.138961 \t NLL Loss: -43.132040\n",
      "Train Epoch: 40 [800/1000 (80%)]\t KLD Loss: 1.295983 \t NLL Loss: -37.520209\n",
      "Train Epoch: 40 [900/1000 (90%)]\t KLD Loss: 1.145475 \t NLL Loss: -36.227495\n",
      "====> Epoch: 40 Average loss: -39.6547\n",
      "====> Test set loss: KLD Loss = 1.2097, NLL Loss = -40.0745 \n"
     ]
    }
   ],
   "source": [
    "train_error = np.zeros([n_epochs , int(train_data.shape[0] / batch_size ) ])\n",
    "train_div = np.zeros([n_epochs , int(train_data.shape[0] / batch_size ) ])\n",
    "test_error , test_div  = np.zeros([n_epochs , test_data.shape[0]]) , np.zeros([n_epochs , test_data.shape[0]]) \n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    #training + testing\n",
    "    tr = train(epoch)\n",
    "    train_error [epoch-1 , :] = tr [0]\n",
    "    train_div [epoch-1 , :] = tr [1] \n",
    "    te = test(epoch)\n",
    "    test_error [epoch-1 , :] = te [0]\n",
    "    test_div [epoch-1 , :] = te [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Latent Representations and Save the Model for later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lat = [ model (Variable(torch.tensor(train_data.iloc[idx,:].values)).reshape(1,-1))[-1] for idx in range(len(train_data)) ]\n",
    "test_lat = [ model (Variable(torch.tensor(test_data.iloc[idx,:].values)).reshape(1,-1))[-1] for idx in range(len(test_data)) ]\n",
    "train_lat = pd.DataFrame(torch.cat(train_lat).cpu().detach().numpy())\n",
    "test_lat = pd.DataFrame(torch.cat(test_lat).cpu().detach().numpy())\n",
    "cols = []\n",
    "for i in range(train_lat.shape[1]):\n",
    "    cols.append(str('Z'+str(i+1)))\n",
    "train_lat.columns = cols\n",
    "test_lat.columns = cols\n",
    "train_lat.to_csv('Training_Data_Sets\\\\latent_50D.csv')\n",
    "test_lat.to_csv('Test_Data_Sets\\\\latent_50D.csv')\n",
    "with open('Models\\\\50D.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.zeros((10000,25))\n",
    "for i in range(len(array)):\n",
    "    array [i] = model(Variable(torch.tensor(np.zeros(50))))[-1].cpu().detach().numpy()\n",
    "array1 = np.zeros((10000,25))\n",
    "for i in range(len(array)):\n",
    "    array1 [i] = model(Variable(torch.tensor(np.ones(50))))[-1].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12781853826622014"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(np.mean(array1, 0)-np.mean(array, 0)))/4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
